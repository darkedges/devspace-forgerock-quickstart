## Labels to apply to all resources
##
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd

## If true, create & use Pod Security Policy resources
## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
podSecurityPolicy:
  enabled: false

genjks:
  image:
    repository: devspace-forgerock-quickstart/genjks
    tag: 7.5.0
    imagePullPolicy: IfNotPresent

fram:
  fbc: true
  image:
    repository: devspace-forgerock-quickstart/am
    tag: 7.5.0
    imagePullPolicy: IfNotPresent
    chroot: false
    runAsUser: 11111
    allowPrivilegeEscalation: true

  # -- Rollback limit
  ##
  revisionHistoryLimit: 10

  # Secrets
  secrets:
    admin:
      # fram.secrets.admin.password -- see docker FRAM_ADMIN_PASSWORD
      password: Passw0rd
    cfgStore:
      # fram.secrets.cfgStore.uid -- see docker FRAM_CFG_STORE_DIR_MGR
      uid: uid=am-config,ou=admins,dc=amconfig
      # fram.secrets.cfgStore.password -- see docker FRAM_CFG_STORE_DIR_MGR_PWD
      password: Passw0rd
    userStore:
      # fram.secrets.userStore.uid -- see docker FRAM_USER_STORE_DIR_MGR
      uid: uid=am-identity-bind-account,ou=admins,ou=identities
      # fram.secrets.userStore.password -- see docker FRAM_USER_STORE_DIR_MGR_PWD
      password: Passw0rd
    ctsStore:
      # fram.secrets.ctsStore.uid -- uid for CTS Store
      uid: uid=openam_cts,ou=admins,ou=famrecords,ou=openam-session,ou=tokens
      # fram.secrets.ctsStore.password -- password for CTS Store
      password: Passw0rd

  # stores
  stores:
    cfgStore:
      # fram.stores.cfgStore.enabled --  config store enabled
      enabled: true
      # fram.stores.cfgStore.enabled --  config store root uffix
      rootSuffix: dc=amconfig
      # fram.stores.cfgStore.enabled --  config store type
      instance: development
    userStore:
      # fram.stores.userStore.enabled --  userStore store enabled
      enabled: true
      # fram.stores.userStore.rootSuffix --  userStore store root uffix
      rootSuffix: ou=identities
      # fram.stores.userStore.instance --  userStore store type
      instance: development
    ctsStore:
      # fram.stores.ctsStore.enabled --  ctsStore store enabled
      enabled: true
      # fram.stores.ctsStore.rootSuffix --  ctsStore store root uffix
      rootSuffix: dc=cts,ou=identities
      # fram.stores.ctsStore.instance --  ctsStore store type
      instance: development

  # define what the default values are for a Volume Claim Template
  volumeClaimTemplates:
    data:
      # fram.volumeClaimTemplates.data.storageClassName -- what storage class to use
      storageClassName:
      # fram.volumeClaimTemplates.data.storageSize -- size to be requested.
      storageSize: 100Mi

  init:
    ## fram.init.env -- list of variables to be added for the init container config map
    env: []

  # rbac
  rbac:
    # fram.rbac.enabled -- whether to create roles and bindings
    enabled: true

  # service account
  serviceAccount:
    # fram.serviceAccount.enabled -- whether to create services accounts
    enabled: true
    automountServiceAccountToken: true
    # fram.serviceAccount.annotations -- list of annotations to attach to a service account
    annotations: {}
    name:

  # fram.services --  map of multiple service details that can be connected to
  services: {}
  #  identity:
  #    waitfor: service
  #    name: fram-development
  #    releaseNameOverride: localdev-fram
  #    port: 1389
  #    ssl: false

  # certificates
  certificate:
    # fram.certificate.enabled -- Wether to attach a certificate to ingress
    enabled: false

  # Ingress
  ingress:
    # fram.ingress.enabled -- Wether to expose the service to ingress
    enabled: false
    tls:
      # fram.ingress.tlsenabled -- Wether to expose TLS
      enabled: false
    # fram.ingress.hosts -- List of hostname to use for ingress
    hosts:
      - am.7f000001.nip.io
    # fram.ingress.configurationsnippet -- Configuration Snippet to add
    configurationsnippet: {}
    backend:
      # fram.ingress.backend.protocol -- scheme being used by the backend application
      protocol: HTTP
      # fram.ingress.backend.port -- port being used by the backend application
      port: 8080
  # fram.useKeystore -- has a keystore been generated and provided
  useKeystore: false
  podManagementPolicy: OrderedReady
  keystore:
    # fram.keystore.password --  Password for the keystore
    password: changeit

  # fram.replicaCount -- number of replicas
  replicaCount: 1

  terminationGracePeriodSeconds: 300

  # -- Use an existing PSP instead of creating one
  existingPsp: ""

  # -- Labels to be added to the controller Deployment or DaemonSet and other resources that do not have option to specify labels
  ##
  labels: {}
  #  keel.sh/policy: patch
  #  keel.sh/trigger: poll

  # -- Required for use with CNI based kubernetes installations (such as ones set up by kubeadm),
  # since CNI and hostport don't mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920
  # is merged
  hostNetwork: false

  # -- Node labels for controller pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector:
    kubernetes.io/os: linux

  # -- The update strategy to apply to the Deployment or DaemonSet
  ##
  updateStrategy: {}
  #  rollingUpdate:
  #    maxUnavailable: 1
  #  type: RollingUpdate

  # -- `minReadySeconds` to avoid killing pods before we are ready
  ##
  minReadySeconds: 0

  # -- Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  affinity: {}

  # -- Security Context policies for controller pods
  # See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for
  # notes on enabling and using sysctls
  ##
  podSecurityContext: {}

  # -- Security Context policies for controller main container.
  # See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for
  # notes on enabling and using sysctls
  ##
  containerSecurityContext: {}

  # -- See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for notes on enabling and using sysctls
  sysctls: {}
  # sysctls:
  #   "net.core.somaxconn": "8192"

  # -- Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  topologySpreadConstraints:
    []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app.kubernetes.io/instance: fram-internal

  resources:
    ##  limits:
    ##    cpu: 100m
    ##    memory: 90Mi
    requests:
      cpu: 100m
      memory: 90Mi

  ## Liveness and readiness probe values
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ##
  ## startupProbe:
  ##   tcpSocket:
  ##     port: 1636
  ##   initialDelaySeconds: 5
  ##   periodSeconds: 5
  ##   timeoutSeconds: 2
  ##   successThreshold: 1
  ##   failureThreshold: 5
  livenessProbe:
    httpGet:
      path: "am/json/health/live"
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 5
  readinessProbe:
    httpGet:
      path: "am/json/health/ready"
      port: 8080
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 5

  # -- Optionally customize the pod dnsConfig.
  dnsConfig: {}

  # -- Optionally customize the pod hostname.
  hostname: {}

  # -- Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.
  # By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller
  # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.
  dnsPolicy: ClusterFirst

  # -- Improve connection draining when ingress controller pod is deleted using a lifecycle hook:
  # With this new hook, we increased the default terminationGracePeriodSeconds from 30 seconds
  # to 300, allowing the draining of connections up to five minutes.
  # If the active connections end before that, the pod will terminate gracefully at that time.
  # To effectively take advantage of this feature, the Configmap feature
  # worker-shutdown-timeout new value is 240s instead of 10s.
  ##
  lifecycle:
    preStop:
      exec:
        command:
          - /opt/fram/docker-entrypoint.sh
          - stop

  minAvailable: 1
  # Mutually exclusive with keda autoscaling
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 11
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
    behavior:
      {}
      # scaleDown:
      #   stabilizationWindowSeconds: 300
      #  policies:
      #   - type: Pods
      #     value: 1
      #     periodSeconds: 180
      # scaleUp:
      #   stabilizationWindowSeconds: 300
      #   policies:
      #   - type: Pods
      #     value: 2
      #     periodSeconds: 60

  autoscalingTemplate: []
  # Custom or additional autoscaling metrics
  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
  # - type: Pods
  #   pods:
  #     metric:
  #       name: nginx_ingress_controller_nginx_process_requests_total
  #     target:
  #       type: AverageValue
  #       averageValue: 10000m

  # Mutually exclusive with hpa autoscaling
  keda:
    apiVersion: "keda.sh/v1alpha1"
    ## apiVersion changes with keda 1.x vs 2.x
    ## 2.x = keda.sh/v1alpha1
    ## 1.x = keda.k8s.io/v1alpha1
    enabled: false
    minReplicas: 1
    maxReplicas: 11
    pollingInterval: 30
    cooldownPeriod: 300
    restoreToOriginalReplicaCount: false
    scaledObject:
      annotations: {}
      # Custom annotations for ScaledObject resource
      #  annotations:
      # key: value
    triggers: []
    #     - type: prometheus
    #       metadata:
    #         serverAddress: http://<prometheus-host>:9090
    #         metricName: http_requests_total
    #         threshold: '100'
    #         query: sum(rate(http_requests_total{deployment="my-deployment"}[2m]))

    behavior: {}
  #     scaleDown:
  #       stabilizationWindowSeconds: 300
  #       policies:
  #       - type: Pods
  #         value: 1
  #         periodSeconds: 180
  #     scaleUp:
  #       stabilizationWindowSeconds: 300
  #       policies:
  #       - type: Pods
  #         value: 2
  #         periodSeconds: 60

  # -- Optional array of imagePullSecrets containing private registry credentials
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  imagePullSecrets: []
  # - name: secretName
